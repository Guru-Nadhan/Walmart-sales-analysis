{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Walmart Inc. is an American multinational retail corporation that operates a chain of hypermarkets, discount department stores, and grocery stores from the United States, headquartered in Bentonville, Arkansas.","metadata":{}},{"cell_type":"markdown","source":"##### Task is to analyze  the sales of  walmart with the data given. (eg: anaglysing weekly sales, which store or department sells most etc.)","metadata":{}},{"cell_type":"markdown","source":"## DATA ANALYSIS AND VIUALIZATION ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.371364Z","iopub.execute_input":"2021-10-03T11:50:46.371756Z","iopub.status.idle":"2021-10-03T11:50:46.377675Z","shell.execute_reply.started":"2021-10-03T11:50:46.371635Z","shell.execute_reply":"2021-10-03T11:50:46.37682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Reading all the csv files as dataframes using pd.read_csv method.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/wallmart-sales-forecast-datasets/train.csv')\ntest_df = pd.read_csv('../input/wallmart-sales-forecast-datasets/test.csv')\nfeatures_df=pd.read_csv('../input/wallmart-sales-forecast-datasets/features.csv')\nstores_df = pd.read_csv('../input/wallmart-sales-forecast-datasets/stores.csv')\n","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.382664Z","iopub.execute_input":"2021-10-03T11:50:46.382942Z","iopub.status.idle":"2021-10-03T11:50:46.592483Z","shell.execute_reply.started":"2021-10-03T11:50:46.382915Z","shell.execute_reply":"2021-10-03T11:50:46.591775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### We can see that the dataframe has weekly  sales data in accordance with store department and date.","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.594113Z","iopub.execute_input":"2021-10-03T11:50:46.594403Z","iopub.status.idle":"2021-10-03T11:50:46.606623Z","shell.execute_reply.started":"2021-10-03T11:50:46.594367Z","shell.execute_reply":"2021-10-03T11:50:46.605663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.608047Z","iopub.execute_input":"2021-10-03T11:50:46.608437Z","iopub.status.idle":"2021-10-03T11:50:46.648111Z","shell.execute_reply.started":"2021-10-03T11:50:46.608399Z","shell.execute_reply":"2021-10-03T11:50:46.647281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Describe the data in train_df, that is mean count etc of the numerical columns in the data frame.","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.649498Z","iopub.execute_input":"2021-10-03T11:50:46.649722Z","iopub.status.idle":"2021-10-03T11:50:46.692326Z","shell.execute_reply.started":"2021-10-03T11:50:46.649683Z","shell.execute_reply":"2021-10-03T11:50:46.691535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### This shows how many null values each column in the data frame has, we can see that there are no null values in any columns of the data frame.","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.693279Z","iopub.execute_input":"2021-10-03T11:50:46.693494Z","iopub.status.idle":"2021-10-03T11:50:46.724099Z","shell.execute_reply.started":"2021-10-03T11:50:46.693468Z","shell.execute_reply":"2021-10-03T11:50:46.723254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Viewing the test_df","metadata":{}},{"cell_type":"code","source":"test_df.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-03T11:50:46.725284Z","iopub.execute_input":"2021-10-03T11:50:46.725535Z","iopub.status.idle":"2021-10-03T11:50:46.737732Z","shell.execute_reply.started":"2021-10-03T11:50:46.725507Z","shell.execute_reply":"2021-10-03T11:50:46.73681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.739144Z","iopub.execute_input":"2021-10-03T11:50:46.739371Z","iopub.status.idle":"2021-10-03T11:50:46.748912Z","shell.execute_reply.started":"2021-10-03T11:50:46.739346Z","shell.execute_reply":"2021-10-03T11:50:46.748285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Describing the data in the test dataframe.","metadata":{}},{"cell_type":"code","source":"test_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.750211Z","iopub.execute_input":"2021-10-03T11:50:46.750507Z","iopub.status.idle":"2021-10-03T11:50:46.775136Z","shell.execute_reply.started":"2021-10-03T11:50:46.75047Z","shell.execute_reply":"2021-10-03T11:50:46.774258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Analysing null values in each column.","metadata":{}},{"cell_type":"code","source":"test_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.776388Z","iopub.execute_input":"2021-10-03T11:50:46.776667Z","iopub.status.idle":"2021-10-03T11:50:46.791329Z","shell.execute_reply.started":"2021-10-03T11:50:46.776631Z","shell.execute_reply":"2021-10-03T11:50:46.790574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Viewing the features dataframe.","metadata":{}},{"cell_type":"code","source":"features_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.793504Z","iopub.execute_input":"2021-10-03T11:50:46.793744Z","iopub.status.idle":"2021-10-03T11:50:46.808554Z","shell.execute_reply.started":"2021-10-03T11:50:46.793693Z","shell.execute_reply":"2021-10-03T11:50:46.807802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Viewing the Info of each column in the features dataframe.","metadata":{}},{"cell_type":"code","source":"features_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.809624Z","iopub.execute_input":"2021-10-03T11:50:46.809903Z","iopub.status.idle":"2021-10-03T11:50:46.827981Z","shell.execute_reply.started":"2021-10-03T11:50:46.80986Z","shell.execute_reply":"2021-10-03T11:50:46.827148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfeatures_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.82908Z","iopub.execute_input":"2021-10-03T11:50:46.829307Z","iopub.status.idle":"2021-10-03T11:50:46.867489Z","shell.execute_reply.started":"2021-10-03T11:50:46.829276Z","shell.execute_reply":"2021-10-03T11:50:46.866724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Finding out about the null values in each column.","metadata":{}},{"cell_type":"code","source":"features_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.868739Z","iopub.execute_input":"2021-10-03T11:50:46.869361Z","iopub.status.idle":"2021-10-03T11:50:46.876932Z","shell.execute_reply.started":"2021-10-03T11:50:46.869319Z","shell.execute_reply":"2021-10-03T11:50:46.876384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Viewing the stores data frame.","metadata":{}},{"cell_type":"code","source":"stores_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.877809Z","iopub.execute_input":"2021-10-03T11:50:46.878003Z","iopub.status.idle":"2021-10-03T11:50:46.887747Z","shell.execute_reply.started":"2021-10-03T11:50:46.877979Z","shell.execute_reply":"2021-10-03T11:50:46.887083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stores_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.888853Z","iopub.execute_input":"2021-10-03T11:50:46.889039Z","iopub.status.idle":"2021-10-03T11:50:46.907503Z","shell.execute_reply.started":"2021-10-03T11:50:46.889018Z","shell.execute_reply":"2021-10-03T11:50:46.906401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Finding the null values in each column.","metadata":{}},{"cell_type":"code","source":"stores_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.908465Z","iopub.execute_input":"2021-10-03T11:50:46.908701Z","iopub.status.idle":"2021-10-03T11:50:46.918202Z","shell.execute_reply.started":"2021-10-03T11:50:46.908667Z","shell.execute_reply":"2021-10-03T11:50:46.917476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Create a new data frame by merging (inner) features data frame to stores data frame  on Store for analysis purpose.","metadata":{}},{"cell_type":"code","source":"dataset_m = features_df.merge(stores_df, how= 'inner', on = 'Store')\ndataset_m.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.919337Z","iopub.execute_input":"2021-10-03T11:50:46.919521Z","iopub.status.idle":"2021-10-03T11:50:46.942886Z","shell.execute_reply.started":"2021-10-03T11:50:46.9195Z","shell.execute_reply":"2021-10-03T11:50:46.94221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Gathering info regarding the columns of the new data frame.","metadata":{}},{"cell_type":"code","source":"dataset_m.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.944051Z","iopub.execute_input":"2021-10-03T11:50:46.944436Z","iopub.status.idle":"2021-10-03T11:50:46.957002Z","shell.execute_reply.started":"2021-10-03T11:50:46.944407Z","shell.execute_reply":"2021-10-03T11:50:46.95621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Convering date to date time object.","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\n#converting date to date time object\ndataset_m['Date'] = pd.to_datetime(dataset_m['Date'])\ntrain_df['Date'] = pd.to_datetime(train_df['Date'])\ntest_df['Date'] = pd.to_datetime(test_df['Date'])\n#Extract week and year from the date and make them into new columns.\ndataset_m['week'] = dataset_m.Date.dt.isocalendar().week\ndataset_m['year'] = dataset_m.Date.dt.isocalendar().year","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:46.958196Z","iopub.execute_input":"2021-10-03T11:50:46.958417Z","iopub.status.idle":"2021-10-03T11:50:47.049731Z","shell.execute_reply.started":"2021-10-03T11:50:46.958391Z","shell.execute_reply":"2021-10-03T11:50:47.049101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_m.tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:47.050781Z","iopub.execute_input":"2021-10-03T11:50:47.051453Z","iopub.status.idle":"2021-10-03T11:50:47.069699Z","shell.execute_reply.started":"2021-10-03T11:50:47.051421Z","shell.execute_reply":"2021-10-03T11:50:47.068795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Creating a new df by merging (inner) the train_df and the new data frame dataset_m on  store, date and IsHoliday columns and sorting the values by Store, department and date.","metadata":{}},{"cell_type":"code","source":"train_df_1 = train_df.merge(dataset_m, how='inner', on=['Store', 'Date', 'IsHoliday']).sort_values(by=['Store','Dept','Date']).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:47.070801Z","iopub.execute_input":"2021-10-03T11:50:47.071009Z","iopub.status.idle":"2021-10-03T11:50:47.347513Z","shell.execute_reply.started":"2021-10-03T11:50:47.070985Z","shell.execute_reply":"2021-10-03T11:50:47.346759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_1.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:47.348558Z","iopub.execute_input":"2021-10-03T11:50:47.349018Z","iopub.status.idle":"2021-10-03T11:50:47.369034Z","shell.execute_reply.started":"2021-10-03T11:50:47.348984Z","shell.execute_reply":"2021-10-03T11:50:47.368142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VISUALIZATIONS ","metadata":{}},{"cell_type":"markdown","source":"#### Defining a function scatter to plot the scatter plot with the data of the column specified as the function parameter in x axis and weekly sales as the y values.","metadata":{}},{"cell_type":"markdown","source":"### 1) scatter plot","metadata":{}},{"cell_type":"code","source":"def scatter(train_df_1, column):\n    #plot the figure\n    plt.figure()\n    #plot the scatter plot with data from the specified column in x axis and weekly sales in y axis\n    plt.scatter(train_df[column], train_df['Weekly_Sales'])\n    #give y label as weekly_sales\n    plt.ylabel('Weekly_Sales')\n    #Give the xlabel as the column specified as parameter in the function\n    plt.xlabel(column)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:47.370339Z","iopub.execute_input":"2021-10-03T11:50:47.370534Z","iopub.status.idle":"2021-10-03T11:50:47.374381Z","shell.execute_reply.started":"2021-10-03T11:50:47.370512Z","shell.execute_reply":"2021-10-03T11:50:47.37367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot a scatter plot using the scatter function a scatter plot of weekly sales with respect to Store\nscatter(train_df_1, 'Store')\n#plot a scatter plot using the scatter function a scatter plot of weekly sales with respect to Department\nscatter(train_df_1, 'Dept')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:47.375464Z","iopub.execute_input":"2021-10-03T11:50:47.37568Z","iopub.status.idle":"2021-10-03T11:50:50.740753Z","shell.execute_reply.started":"2021-10-03T11:50:47.375649Z","shell.execute_reply":"2021-10-03T11:50:50.740176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see scatter plots of weekly sales with respect to stores and department, we can see that departments between 60 and 80 are performing better.","metadata":{}},{"cell_type":"markdown","source":"### 2) Three line plots for weekly sales for eact year","metadata":{}},{"cell_type":"code","source":"#Filtering out mean weekly sales for the year 2011\nweekly_sales_2011 = train_df_1[train_df_1['year'] ==  2011]['Weekly_Sales'].groupby(train_df_1['week']).mean()\n#Plot a line plot with week on x axis and sales for that particular week of the filtered year in y axis\nsns.lineplot(weekly_sales_2011.index, weekly_sales_2011.values,color='red')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-03T11:50:50.741754Z","iopub.execute_input":"2021-10-03T11:50:50.742089Z","iopub.status.idle":"2021-10-03T11:50:51.032383Z","shell.execute_reply.started":"2021-10-03T11:50:50.742062Z","shell.execute_reply":"2021-10-03T11:50:51.030814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the line plot for weekly sales in the year 2011","metadata":{}},{"cell_type":"code","source":"#Filtering out mean weekly sales for the year 2010\nweekly_sales_2010 = train_df_1[train_df_1['year'] ==  2010]['Weekly_Sales'].groupby(train_df_1['week']).mean()\n#Plot a line plot with week on x axis and sales for that particular week of the filtered year in y axis\nsns.lineplot(weekly_sales_2010.index, weekly_sales_2010.values)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:51.033419Z","iopub.execute_input":"2021-10-03T11:50:51.033616Z","iopub.status.idle":"2021-10-03T11:50:51.315791Z","shell.execute_reply.started":"2021-10-03T11:50:51.033593Z","shell.execute_reply":"2021-10-03T11:50:51.314908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the line plot for weekly sales in the year 2010","metadata":{}},{"cell_type":"code","source":"#Filtering out mean weekly sales for the year 2012\nweekly_sales_2012 = train_df_1[train_df_1['year'] ==  2012]['Weekly_Sales'].groupby(train_df_1['week']).mean()\n#Plot a line plot with week on x axis and sales for that particular week of the filtered year in y axis\nsns.lineplot(weekly_sales_2012.index, weekly_sales_2012.values,color='green')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:51.316986Z","iopub.execute_input":"2021-10-03T11:50:51.317177Z","iopub.status.idle":"2021-10-03T11:50:51.579703Z","shell.execute_reply.started":"2021-10-03T11:50:51.317155Z","shell.execute_reply":"2021-10-03T11:50:51.578948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the line plot for weekly sales in the year 2012","metadata":{}},{"cell_type":"markdown","source":"### 3) Combined line plots for each year (2010,2011,2012)","metadata":{}},{"cell_type":"markdown","source":"#### Plotting the weekly sales for years 2010,2011,2012 on a single plot to compare the sales values with respect to week on each of these specified year for comparing the weekly sales in each year.","metadata":{}},{"cell_type":"code","source":"#plotting a figure \nplt.figure(figsize= (20, 10))\n#Plotting three line plots for weekly sales for each year\nsns.lineplot(weekly_sales_2010.index, weekly_sales_2010.values)\nsns.lineplot(weekly_sales_2011.index, weekly_sales_2011.values)\nsns.lineplot(weekly_sales_2012.index, weekly_sales_2012.values)\nplt.grid()\n#plot labels and legends\nplt.xticks(np.arange(1,60, step= 1))\nplt.title('Average Weekly Sales Per Year', fontsize = 20)\nplt.xlabel('Week', fontsize = 16)\nplt.ylabel('Sales', fontsize = 16)\nplt.legend(['2010', '2011','2012'], loc = 'best', fontsize = 16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:51.582954Z","iopub.execute_input":"2021-10-03T11:50:51.583184Z","iopub.status.idle":"2021-10-03T11:50:52.994799Z","shell.execute_reply.started":"2021-10-03T11:50:51.583155Z","shell.execute_reply":"2021-10-03T11:50:52.9939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can see from the combined line lot that on week 51 of 2010 we have the maximum sale ","metadata":{}},{"cell_type":"markdown","source":"Line plots between week and weekly sales is a good way to compare sales for each year.","metadata":{}},{"cell_type":"markdown","source":"### 4) Histogram / Normal Distribution ","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\n#Creating a distribution for weekly sales in the dataframe train_df\nsns.distplot(train_df['Weekly_Sales'],color='g')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:52.995857Z","iopub.execute_input":"2021-10-03T11:50:52.996063Z","iopub.status.idle":"2021-10-03T11:50:55.17348Z","shell.execute_reply.started":"2021-10-03T11:50:52.996039Z","shell.execute_reply":"2021-10-03T11:50:55.172857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\nprint(\"Skewness: \", train_df['Weekly_Sales'].skew()) #skewness\n#Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers\nprint(\"Kurtosis: \", train_df['Weekly_Sales'].kurt()) #kurtosis","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:55.174411Z","iopub.execute_input":"2021-10-03T11:50:55.174786Z","iopub.status.idle":"2021-10-03T11:50:55.191375Z","shell.execute_reply.started":"2021-10-03T11:50:55.174758Z","shell.execute_reply":"2021-10-03T11:50:55.190071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above code is to determine the skew and kurtosis of the normal distribution.","metadata":{}},{"cell_type":"markdown","source":"### 5) Box Plot","metadata":{}},{"cell_type":"code","source":"#concatinating the type and column from store_df together to find the relation between stores falling under each type and size.\ndata = pd.concat([stores_df['Type'], stores_df['Size']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\n#Plot a box plot to understand the relation between type and size of stores.\nfig = sns.boxplot(x='Type', y='Size', data=data)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:55.192913Z","iopub.execute_input":"2021-10-03T11:50:55.193184Z","iopub.status.idle":"2021-10-03T11:50:55.427867Z","shell.execute_reply.started":"2021-10-03T11:50:55.193155Z","shell.execute_reply":"2021-10-03T11:50:55.427187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above box plots we can see that Type A plots are the largest, ie has the largest size","metadata":{}},{"cell_type":"markdown","source":"### 6) Bar Chart On Data Frame","metadata":{}},{"cell_type":"code","source":"weekly_sales_store = train_df['Weekly_Sales'].groupby(train_df['Store']).mean()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:55.429012Z","iopub.execute_input":"2021-10-03T11:50:55.429455Z","iopub.status.idle":"2021-10-03T11:50:55.442286Z","shell.execute_reply.started":"2021-10-03T11:50:55.429421Z","shell.execute_reply":"2021-10-03T11:50:55.441201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Grouping the train_df by store and computing the mean of weekly_sales for each store.","metadata":{}},{"cell_type":"code","source":"weekly_sales_store.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:55.443603Z","iopub.execute_input":"2021-10-03T11:50:55.444118Z","iopub.status.idle":"2021-10-03T11:50:55.451653Z","shell.execute_reply.started":"2021-10-03T11:50:55.444073Z","shell.execute_reply":"2021-10-03T11:50:55.450566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Converting the data to a pandas dataframe","metadata":{}},{"cell_type":"code","source":"weekly_sales_store_df=pd.DataFrame(weekly_sales_store)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:55.453113Z","iopub.execute_input":"2021-10-03T11:50:55.453418Z","iopub.status.idle":"2021-10-03T11:50:55.463578Z","shell.execute_reply.started":"2021-10-03T11:50:55.453389Z","shell.execute_reply":"2021-10-03T11:50:55.462572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Sorting the data frame with respect to weekly sales in Descending order and plotting a var graph on the data frame itself to get a idea about the data.","metadata":{}},{"cell_type":"code","source":"weekly_sales_store_df.sort_values(\"Weekly_Sales\",ascending=False).style.bar(align='left',width=98,color='#FFD200')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-03T11:50:55.464882Z","iopub.execute_input":"2021-10-03T11:50:55.465256Z","iopub.status.idle":"2021-10-03T11:50:55.487624Z","shell.execute_reply.started":"2021-10-03T11:50:55.465224Z","shell.execute_reply":"2021-10-03T11:50:55.486678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can see that Store number 20 has the highest weekly sales (20508.301592)","metadata":{}},{"cell_type":"markdown","source":"#### Sorting the data frame with respect to weekly sales in Ascending order and plotting a var graph on the data frame itself to get a idea about the data.","metadata":{}},{"cell_type":"code","source":"weekly_sales_store_df.sort_values(\"Weekly_Sales\").style.bar(align='left',width=98,color='#ff00bf')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-03T11:50:55.48926Z","iopub.execute_input":"2021-10-03T11:50:55.490048Z","iopub.status.idle":"2021-10-03T11:50:55.507481Z","shell.execute_reply.started":"2021-10-03T11:50:55.48997Z","shell.execute_reply":"2021-10-03T11:50:55.506611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can see that Store number 5 has the least weekly sales (5053.415813)","metadata":{}},{"cell_type":"markdown","source":"### 7) Bar graph fpr mean weekly sales for each store.","metadata":{}},{"cell_type":"markdown","source":"## Plotting a bar graph with store number as x values and mean weekly sales for that store as the y values.","metadata":{}},{"cell_type":"code","source":"#Plotting the figure.\nplt.figure(figsize= (25, 12))\n#Creating a barplot with stores in x axis and mean weekly sales for each store as y values.\nsns.barplot(weekly_sales_store.index, weekly_sales_store.values,palette='dark')\n#plot a grid\nplt.grid()\n#Give title,x and y labels for the plot\nplt.title('Average Weekly Sales Per Store', fontsize = 20)\nplt.xlabel('Store Number', fontsize = 16)\nplt.ylabel('Sales Per Store ', fontsize = 16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:55.508524Z","iopub.execute_input":"2021-10-03T11:50:55.508833Z","iopub.status.idle":"2021-10-03T11:50:56.270771Z","shell.execute_reply.started":"2021-10-03T11:50:55.508801Z","shell.execute_reply":"2021-10-03T11:50:56.269922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1)We can see that this plot is a great way to analyse how each store is performing sales wise.\n\n2)From the plot we can see that store number 20 has the highesst mean weekly sales, followed by store 4 and then store 14.\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Grouping the train_df by Department and computing the mean of weekly_sales for each Department.","metadata":{}},{"cell_type":"markdown","source":"### 8) Bar chart on Data Frame.","metadata":{}},{"cell_type":"code","source":"weekly_sales_Dept = train_df['Weekly_Sales'].groupby(train_df['Dept']).mean()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:56.271969Z","iopub.execute_input":"2021-10-03T11:50:56.272194Z","iopub.status.idle":"2021-10-03T11:50:56.284841Z","shell.execute_reply.started":"2021-10-03T11:50:56.272169Z","shell.execute_reply":"2021-10-03T11:50:56.2839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"converting it to a pandas dataframe\n","metadata":{}},{"cell_type":"code","source":"weekly_sales_Dept_df= pd.DataFrame(weekly_sales_Dept)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:56.286389Z","iopub.execute_input":"2021-10-03T11:50:56.286636Z","iopub.status.idle":"2021-10-03T11:50:56.291592Z","shell.execute_reply.started":"2021-10-03T11:50:56.286607Z","shell.execute_reply":"2021-10-03T11:50:56.290782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weekly_sales_Dept_df","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:56.293056Z","iopub.execute_input":"2021-10-03T11:50:56.293351Z","iopub.status.idle":"2021-10-03T11:50:56.309593Z","shell.execute_reply.started":"2021-10-03T11:50:56.293314Z","shell.execute_reply":"2021-10-03T11:50:56.308834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Sorting the data frame with respect to weekly sales in Descending order and plotting a var graph on the data frame itself to get a idea about the data.","metadata":{}},{"cell_type":"code","source":"weekly_sales_Dept_df.sort_values(\"Weekly_Sales\",ascending=False).style.bar(align='left',width=98,color='#d65f5f')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-03T11:50:56.310778Z","iopub.execute_input":"2021-10-03T11:50:56.311139Z","iopub.status.idle":"2021-10-03T11:50:56.33397Z","shell.execute_reply.started":"2021-10-03T11:50:56.311107Z","shell.execute_reply":"2021-10-03T11:50:56.332857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" We can see that Department number 92 has the largest weekly sales.","metadata":{}},{"cell_type":"markdown","source":"####  Sorting the data frame with respect to weekly sales in Ascending order and plotting a var graph on the data frame itself to get a idea about the data.","metadata":{}},{"cell_type":"code","source":"weekly_sales_Dept_df.sort_values(\"Weekly_Sales\").style.bar(align='left',width=98,color='#bfff00')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-03T11:50:56.335297Z","iopub.execute_input":"2021-10-03T11:50:56.335596Z","iopub.status.idle":"2021-10-03T11:50:56.362273Z","shell.execute_reply.started":"2021-10-03T11:50:56.335557Z","shell.execute_reply":"2021-10-03T11:50:56.361477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We can see that department 47 has a cumulative negative value for weekly sales followed by department 43 which has a very low weekly sale of 1.193333","metadata":{}},{"cell_type":"markdown","source":"### 9)Bar graph fpr mean weekly sales for each Department.","metadata":{}},{"cell_type":"markdown","source":"#### Plotting a bar graph with Department number as x values and mean weekly sales for that store as the y values.","metadata":{}},{"cell_type":"code","source":"#Plot a figure\nplt.figure(figsize= (25, 12))\n#Creating a barplot with department in x axis and mean weekly sales for each store as y values.\nsns.barplot(weekly_sales_Dept.index, weekly_sales_Dept.values)\n#plot a grid\nplt.grid()\n#Give title,x and y labels for the plot\nplt.title('Average Weekly Sales Per Department', fontsize = 20)\nplt.xlabel('Department', fontsize = 16)\nplt.ylabel('Sales', fontsize = 16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:56.363725Z","iopub.execute_input":"2021-10-03T11:50:56.36417Z","iopub.status.idle":"2021-10-03T11:50:58.379725Z","shell.execute_reply.started":"2021-10-03T11:50:56.36413Z","shell.execute_reply":"2021-10-03T11:50:58.379159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that weekly sales is highest for department 92.","metadata":{}},{"cell_type":"markdown","source":"### 10) Series of line plots for mean sales with respect to Departments.","metadata":{}},{"cell_type":"markdown","source":"#### Creating line plots of mean weekly sales for each department in the dataframe.","metadata":{}},{"cell_type":"code","source":"#Creating a data frame grouped by grouping the train_df on department and date.\ngrouped=train_df.groupby(['Dept','Date']).mean().reset_index()\n\n#Filtering out Department, Date, Weekly sales from the new data frame grouped.\ndata=grouped[['Dept','Date','Weekly_Sales']]\n\n#Finding out unique values in the department column.\ndept=train_df['Dept'].unique()\n#Sort the unique values in the column\ndept.sort()\n#Divide the departments into 4 batches\ndept_1=dept[0:20]\ndept_2=dept[20:40]\ndept_3=dept[40:60]\ndept_4=dept[60:]\n\n#Create subplots with 2 rows and 2 columns and adjusting the spacing between the subplots.\nfig, ax = plt.subplots(2,2,figsize=(20,10))\nfig.subplots_adjust(wspace=0.5, hspace=0.5)\nfig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)\n\n\n#For each batch containg departments and for each department in each batch plot the weekly sales in accordance with date.\nfor i in dept_1 :\n    data_1=data[data['Dept']==i]\n    ax[0,0].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')\n\nfor i in dept_2 :\n    data_1=data[data['Dept']==i]\n    ax[0,1].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')\n    \nfor i in dept_3 :\n    data_1=data[data['Dept']==i]\n    ax[1,0].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')    \n\nfor i in dept_4 :\n    data_1=data[data['Dept']==i]\n    ax[1,1].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')        \n    \n    \n#Give each subplot a suitable title    \nax[0,0].set_title('Mean sales record by department (0-19)')\nax[0,1].set_title('Mean sales record by department (20-39)')\nax[1,0].set_title('Mean sales record by department (40-59)')\nax[1,1].set_title('Mean sales record by department (60-)')\n\n#Give each Subplot y and x labels\nax[0,0].set_ylabel('Mean sales With Respect To Specified Departments')\nax[0,0].set_xlabel('Date')\nax[0,1].set_ylabel('Mean sales With Respect To Specified Departments')\nax[0,1].set_xlabel('Date')\nax[1,0].set_ylabel('Mean sales With Respect To Specified Departments')\nax[1,0].set_xlabel('Date')\nax[1,1].set_ylabel('Mean sales With Respect To Specified Departments')\nax[1,1].set_xlabel('Date')\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:58.380674Z","iopub.execute_input":"2021-10-03T11:50:58.380994Z","iopub.status.idle":"2021-10-03T11:50:59.828353Z","shell.execute_reply.started":"2021-10-03T11:50:58.380968Z","shell.execute_reply":"2021-10-03T11:50:59.827674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1)The sales level is different by department \n\n\n2)There is some peaked points around January and May, thus indicating high sales during those times.\n\nWe can see that departments and date are good features to predict the sales.\n","metadata":{}},{"cell_type":"markdown","source":"### 11) Histogram / Normal Distribution","metadata":{}},{"cell_type":"markdown","source":"### Plotting the normal distribution where sales is greator than zero.","metadata":{}},{"cell_type":"code","source":"#Data for sales over zero\ntrain_over_zero_sales=train_df[train_df['Weekly_Sales']>0]\n#Data for sales less than zero\ntrain_below_zero_sales=train_df[train_df['Weekly_Sales']<=0]\n#Natural log of data for sales over zero\nsales_over_zero_sales = np.log1p(train_over_zero_sales['Weekly_Sales'])\n#plotting a histogram and normal distribution for data with sales over zero.\nf, ax = plt.subplots(figsize=(8, 6))\nsns.distplot(sales_over_zero_sales)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:50:59.829353Z","iopub.execute_input":"2021-10-03T11:50:59.829662Z","iopub.status.idle":"2021-10-03T11:51:02.332584Z","shell.execute_reply.started":"2021-10-03T11:50:59.829634Z","shell.execute_reply":"2021-10-03T11:51:02.33182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Skewness: \", sales_over_zero_sales.skew()) #skewness\nprint(\"Kurtosis: \", sales_over_zero_sales.kurt()) #kurtosis","metadata":{"execution":{"iopub.status.busy":"2021-10-03T11:51:02.333661Z","iopub.execute_input":"2021-10-03T11:51:02.333891Z","iopub.status.idle":"2021-10-03T11:51:02.347794Z","shell.execute_reply.started":"2021-10-03T11:51:02.333866Z","shell.execute_reply":"2021-10-03T11:51:02.346887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"### 12) Heatmap","metadata":{}},{"cell_type":"markdown","source":"### Plotting a heatmap to determine the correlation between columns in the data frame.","metadata":{}},{"cell_type":"code","source":"sns.set(style = 'white')\n\n\nfig, ax = plt.subplots(figsize= (25, 15))\nsns.heatmap(train_df_1.corr(), cmap = 'icefire', vmax = 0.3, center = 0, square = True, linewidth= 0.5, cbar_kws = {'shrink': 0.5}, annot = True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-03T11:51:02.349146Z","iopub.execute_input":"2021-10-03T11:51:02.349566Z","iopub.status.idle":"2021-10-03T11:51:04.43382Z","shell.execute_reply.started":"2021-10-03T11:51:02.349533Z","shell.execute_reply":"2021-10-03T11:51:04.432957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are various column that have very week correlation and thus its always better to drop these columns before modelling since that columns dosent contribute much to the process.\n\n-------------------------------------------------------------------------------------------------------------------------------\n\nAlso,some columns are strongly correlated . One of them must be dropped else they would carry similar information to the model. ","metadata":{}}]}